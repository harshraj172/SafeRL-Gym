[1mdiff --git a/src/agent/ppo_llm.py b/src/agent/ppo_llm.py[m
[1mindex 66c48e7..2e4610b 100644[m
[1m--- a/src/agent/ppo_llm.py[m
[1m+++ b/src/agent/ppo_llm.py[m
[36m@@ -86,12 +86,13 @@[m [mclass ActorNetwork(nn.Module):[m
         )[m
         state_tokens = {k: v.to(device) for k, v in state_tokens.items()}[m
         action_tokens = {k: v.to(device) for k, v in action_tokens.items()}[m
[31m-        action_tokens["input_ids"] = action_tokens["input_ids"][:, 1:] [m
[32m+[m[32m        action_tokens["input_ids"] = action_tokens["input_ids"][:, 1:][m
         eos_token_id = self.tokenizer.eos_token_id[m
[31m-        eos_tensor = torch.full((batch_size, 1), eos_token_id, dtype=torch.long, device=device)[m
[32m+[m[32m        eos_tensor = torch.full([m
[32m+[m[32m            (batch_size, 1), eos_token_id, dtype=torch.long, device=device[m
[32m+[m[32m        )[m
         action_tokens["input_ids"] = torch.cat([m
[31m-            (action_tokens["input_ids"], eos_tensor),[m
[31m-            dim=1[m
[32m+[m[32m            (action_tokens["input_ids"], eos_tensor), dim=1[m
         )[m
         # tokenized_length = action_tokens["attention_mask"].sum(dim=1)[m
         # action_tokens["input_ids"][torch.arange(batch_size, device=device), tokenized_length - 1] = ([m
[36m@@ -125,7 +126,7 @@[m [mclass ActorNetwork(nn.Module):[m
         batch_indices = torch.arange(action_token_indices.shape[0], device=device)[m
         token_indices = torch.arange(m, device=device)[m
         # action_token_logprobs_ = next_token_logprobs[[m
[31m-            # batch_indices.unsqueeze(1), token_indices.unsqueeze(0), action_token_indices[m
[32m+[m[32m        # batch_indices.unsqueeze(1), token_indices.unsqueeze(0), action_token_indices[m
         # ][m
         action_token_logits = next_token_logits[[m
             batch_indices.unsqueeze(1), token_indices.unsqueeze(0), action_token_indices[m
[36m@@ -147,7 +148,6 @@[m [mclass ActorNetwork(nn.Module):[m
         logits = self.forward(state, actions, device)[m
         return torch.distributions.Categorical(logits=logits)[m
 [m
[31m-[m
     def create_conditional_logprobs([m
         self, state: str, actions: list[str], device, probs=False, requires_grad=True[m
     ) -> torch.TensorType:[m
[36m@@ -174,57 +174,89 @@[m [mclass ActorNetwork(nn.Module):[m
 [m
 class CriticNetwork(nn.Module):[m
     """[m
[31m-    Critic network for PPO-LLM agent.[m
[31m-    Estimates the value of a given state using a language model and a small neural network.[m
[32m+[m[32m    Enhanced Critic network for PPO-LLM agent with GRU encoders.[m
[32m+[m[32m    Estimates the value of a given state using multiple GRU encoders for different state aspects.[m
     """[m
 [m
[31m-    def __init__(self, model, tokenizer, input_dim):[m
[32m+[m[32m    def __init__(self, tokenizer, embedding, hidden_dim=128):[m
         super().__init__()[m
[31m-        self.model = model[m
         self.tokenizer = tokenizer[m
[31m-        self.attn_proj = nn.Linear(model.config.hidden_size, 1, bias=False)[m
[31m-        self.simple_nn = nn.Sequential([m
[31m-            nn.Flatten(),[m
[31m-            nn.Linear(input_dim, 64),[m
[32m+[m[32m        # Language model for embeddings[m
[32m+[m[32m        self.embedding = embedding[m
[32m+[m[32m        embedding_dim = self.embedding.get_input_embeddings().embedding_dim[m
[32m+[m[32m        # GRU encoders for different state aspects (similar to DRRN)[m
[32m+[m[32m        self.state_encoder = nn.GRU(embedding_dim, hidden_dim, num_layers=3)[m
[32m+[m
[32m+[m[32m        # Value estimation layers[m
[32m+[m[32m        self.hidden = nn.Sequential([m
[32m+[m[32m            nn.Linear(hidden_dim, hidden_dim),[m
             nn.ReLU(),[m
[31m-            nn.Linear(64, 64),[m
[32m+[m[32m            nn.Linear(hidden_dim, hidden_dim),[m
             nn.ReLU(),[m
[31m-            nn.Linear(64, 1),[m
         )[m
[32m+[m[32m        self.value_estimator = nn.Linear(hidden_dim, 1)[m
[32m+[m[32m        self.hidden_dim = hidden_dim[m
 [m
     def _set_device(self, device):[m
         """[m
[31m-        Move the model and value head to the specified device.[m
[32m+[m[32m        Move the model components to the specified device.[m
         Args:[m
             device: torch.device to move the model to.[m
         """[m
[31m-        self.model = self.model.to(device)[m
[31m-        self.simple_nn = self.simple_nn.to(device)[m
[31m-        self.attn_proj = self.attn_proj.to(device)[m
[32m+[m[32m        self.embedding = self.embedding.to(device)[m
[32m+[m[32m        self.state_encoder = self.state_encoder.to(device)[m
[32m+[m[32m        self.hidden = self.hidden.to(device)[m
[32m+[m[32m        self.value_estimator = self.value_estimator.to(device)[m
[32m+[m
[32m+[m[32m    def packed_rnn(self, x, rnn, device):[m
[32m+[m[32m        """[m
[32m+[m[32m        Runs the provided rnn on the input x. Takes care of packing/unpacking.[m
[32m+[m[32m        x: list of unpadded input sequences or single sequence[m
[32m+[m[32m        Returns a tensor of size: batch_size x hidden_dim[m
[32m+[m[32m        """[m
 [m
[31m-    def forward(self, state: str, device):[m
[32m+[m[32m        # strings[m
[32m+[m
[32m+[m[32m        inputs = self.tokenizer(x, return_tensors="pt", padding=True, truncation=True)[m
[32m+[m[32m        inputs = {k: v.to(device) for k, v in inputs.items()}[m
[32m+[m[32m        embed = self.embedding(**inputs)["last_hidden_state"].permute(1, 0, 2).detach()[m
[32m+[m[32m        out, _ = rnn(embed)[m
[32m+[m
[32m+[m[32m        # Get the last output for each sequence[m
[32m+[m[32m        if len(out.shape) == 3:[m
[32m+[m[32m            # Take the last timestep output[m
[32m+[m[32m            out = out[-1]  # [batch, hidden_dim][m
[32m+[m
[32m+[m[32m        return out[m
[32m+[m
[32m+[m[32m    def forward(self, state_data, device):[m
         """[m
[31m-        Estimate the value of the given state.[m
[32m+[m[32m        Estimate the value of the given state using GRU encoders.[m
         Args:[m
[31m-            state: Input state as a string.[m
[32m+[m[32m            state_data: string (single state description)[m
             device: torch.device to run computation on.[m
         Returns:[m
             torch.Tensor: Estimated value of the state.[m
         """[m
         self._set_device(device)[m
[31m-        assert self.model.device.type == device.type, ([m
[31m-            f"Critic network device type {self.model.device.type} does not match expected device type {device.type}"[m
[31m-        )[m
[31m-        inputs = self.tokenizer(state, return_tensors="pt")[m
[31m-        inputs = {k: v.to(device) for k, v in inputs.items()}[m
[31m-        with torch.no_grad():[m
[31m-            outputs = self.model(**inputs, output_hidden_states=True)[m
[31m-        last_hidden = outputs.hidden_states[-1]  # [batch, seq_len, hidden][m
[31m-        scores = self.attn_proj(last_hidden)  # [batch, seq_len, 1][m
[31m-        weights = torch.softmax(scores.squeeze(-1), dim=1)  # [batch, seq_len][m
[31m-        pooled = torch.einsum("bsh,bs->bh", last_hidden, weights.to(device))[m
[31m-        x = self.simple_nn(pooled)[m
[31m-        return x.flatten()[0][m
[32m+[m
[32m+[m[32m        # Handle different input formats[m
[32m+[m[32m        state_out = self.packed_rnn(state_data, self.state_encoder, device)[m
[32m+[m
[32m+[m[32m        # state_out = state_out.sum(dim=-1) / state_out.shape[-1]  # Average over the hidden dimension[m
[32m+[m
[32m+[m[32m        # Ensure all outputs are on the correct device[m
[32m+[m[32m        state_out = state_out.to(device)[m
[32m+[m
[32m+[m[32m        # Handle batch dimension[m
[32m+[m[32m        if len(state_out.shape) == 1:[m
[32m+[m[32m            state_out = state_out.unsqueeze(0)[m
[32m+[m
[32m+[m[32m        # Pass through final layers[m
[32m+[m[32m        z = self.hidden(state_out)[m
[32m+[m[32m        value = self.value_estimator(z)[m
[32m+[m
[32m+[m[32m        return value.flatten()[0][m
 [m
 [m
 class PPOLLMAgent(BaseAgent):[m
[36m@@ -261,17 +293,30 @@[m [mclass PPOLLMAgent(BaseAgent):[m
             if hasattr(args, "actor_model")[m
             else AutoModel.from_pretrained(args.lm_name)[m
         )[m
[31m-        critic_model = ([m
[31m-            args.critic_model[m
[31m-            if hasattr(args, "critic_model")[m
[31m-            else AutoModel.from_pretrained(args.lm_name)[m
[31m-        )[m
[32m+[m[32m        # critic_model = ([m
[32m+[m[32m        #     args.critic_model[m
[32m+[m[32m        #     if hasattr(args, "critic_model")[m
[32m+[m[32m        #     else AutoModel.from_pretrained(args.lm_name)[m
[32m+[m[32m        # )[m
 [m
         self.actor = ActorNetwork(actor_model, self.tokenizer)[m
[32m+[m[32m        # self.critic = CriticNetwork([m
[32m+[m[32m        #     critic_model, self.tokenizer, input_dim=actor_model.config.hidden_size[m
[32m+[m[32m        # )[m
[32m+[m[32m        # self.critic = CriticNetwork(critic_model, self.tokenizer)[m
[32m+[m[41m        [m
[32m+[m[32m        embedding = AutoModel.from_pretrained([m
[32m+[m[32m            "microsoft/deberta-v3-xsmall", output_hidden_states=True[m
[32m+[m[32m        )[m
[32m+[m[41m        [m
         self.critic = CriticNetwork([m
[31m-            critic_model, self.tokenizer, input_dim=actor_model.config.hidden_size[m
[32m+[m[32m            embedding=embedding,[m
[32m+[m[32m            tokenizer=AutoTokenizer.from_pretrained([m
[32m+[m[32m                "microsoft/deberta-v3-xsmall", model_max_length=512[m
[32m+[m[32m            ),[m
         )[m
 [m
[32m+[m[32m        # remove this[m
         self.env = args.env[m
 [m
         self.gamma = kwargs.get("gamma", 0.99)[m
[36m@@ -382,7 +427,7 @@[m [mclass PPOLLMAgent(BaseAgent):[m
         self.completed_episodes[-1].append(ppo_llm_transition)[m
         if transition.done or len(self.episode_buffer) >= self.max_segment_length:[m
             self.completed_episodes.append([])[m
[31m-            self.episode_buffer = [][m
[32m+[m[32m            self.episode_buffer[env_idx] = [][m
 [m
     def update(self):[m
         """[m
[1mdiff --git a/src/train.py b/src/train.py[m
[1mindex e911994..092e43f 100644[m
[1m--- a/src/train.py[m
[1m+++ b/src/train.py[m
[36m@@ -7,7 +7,7 @@[m [mUSAGE:[m
     python -m src.train --max_steps 10000 --game kung-fu --agent_type DRRN --output_dir ./results/Machiavelli/DRRN/microsoft_deberta-v3-xsmall/kung-fu[m
 [m
     PPO LLM: [m
[31m-    python -m src.train --max_steps 1000 --game kung-fu --agent_type PPO_LLM --lm_name google/gemma-3-1b-it --output_dir ./results/Machiavelli/PPO_LLM/google_gemma-3-1b-it/kung-fu[m
[32m+[m[32m    python -m src.train --max_steps 10000 --game kung-fu --agent_type PPO_LLM --lm_name google/gemma-3-1b-it --output_dir ./results/Machiavelli/PPO_LLM/google_gemma-3-1b-it/kung-fu[m
 """[m
 [m
 import os[m
[36m@@ -206,7 +206,7 @@[m [mdef train([m
         [m
         # Advance[m
         obs, states, valid_ids = next_obs, next_states, next_valids[m
[31m-[m
[32m+[m[32m        print(f"Valid actions: {[a for a in info['game_state']['choice_texts'] for info in infos]}")[m
         if step % log_freq == 0:[m
             tb.logkv("FPS", round((step * args.num_envs) / (time.time() - start), 2))[m
             tb.logkv("Max score seen", max_score)[m
[1mdiff --git a/tests/agent/test_ppo_llm.py b/tests/agent/test_ppo_llm.py[m
[1mindex 77d3084..10dbb38 100644[m
[1m--- a/tests/agent/test_ppo_llm.py[m
[1m+++ b/tests/agent/test_ppo_llm.py[m
[36m@@ -10,6 +10,7 @@[m [mfrom types import SimpleNamespace[m
 import torch[m
 from transformers import PreTrainedTokenizerBase, PretrainedConfig, PreTrainedModel[m
 from transformers import BatchEncoding[m
[32m+[m[32mfrom transformers import AutoModel, AutoTokenizer[m
 from src.env.machiavelli.machiavelli_env import MachiavelliEnv[m
 from src.agent.ppo_llm import ActorNetwork, CriticNetwork, PPOLLMAgent[m
 [m
[36m@@ -170,7 +171,10 @@[m [mdef test_mock_actornetwork_forward(mock_resources):[m
 [m
 def test_mock_criticnetwork_forward(mock_resources):[m
     device, tokenizer, model, config = mock_resources[m
[31m-    critic = CriticNetwork(model, tokenizer, config.hidden_size)[m
[32m+[m[32m     # input_dim = config.hidden_size[m
[32m+[m[32m    embedding = AutoModel.from_pretrained('microsoft/deberta-v3-xsmall', output_hidden_states=True)[m
[32m+[m
[32m+[m[32m    critic = CriticNetwork(embedding=embedding, tokenizer=tokenizer)[m
     state = "Once upon a time , there was a king ."[m
     output = critic(state, device)[m
     assert isinstance(output, torch.Tensor)[m
[36m@@ -203,8 +207,10 @@[m [mdef test_gemma_actornetwork_conditional_logprobs(mock_resources):[m
 [m
 def test_mock_criticnetwork_repr(mock_resources):[m
     _, tokenizer, model, config = mock_resources[m
[31m-    input_dim = config.hidden_size[m
[31m-    critic = CriticNetwork(model, tokenizer, input_dim)[m
[32m+[m[32m    # input_dim = config.hidden_size[m
[32m+[m[32m    embedding = AutoModel.from_pretrained('microsoft/deberta-v3-xsmall', output_hidden_states=True)[m
[32m+[m
[32m+[m[32m    critic = CriticNetwork(embedding=embedding, tokenizer=tokenizer)[m
     summary = str(critic)[m
     print(summary)[m
     assert "CriticNetwork" in summary or "simple_nn" in summary[m
[36m@@ -212,8 +218,10 @@[m [mdef test_mock_criticnetwork_repr(mock_resources):[m
 [m
 def test_gemma_criticnetwork_forward(mock_resources):[m
     device, tokenizer, model, config = mock_resources[m
[31m-    input_dim = config.hidden_size[m
[31m-    critic = CriticNetwork(model, tokenizer, input_dim)[m
[32m+[m[32m     # input_dim = config.hidden_size[m
[32m+[m[32m    embedding = AutoModel.from_pretrained('microsoft/deberta-v3-xsmall', output_hidden_states=True)[m
[32m+[m
[32m+[m[32m    critic = CriticNetwork(embedding=embedding, tokenizer=tokenizer)[m
     state = "Once upon a time, there was a king."[m
     output = critic(state, device)[m
     print(output)[m
